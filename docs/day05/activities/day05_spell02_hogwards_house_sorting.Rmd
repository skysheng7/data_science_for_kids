---
title: "ğŸ”® Day 5 - Spell 2: Hogwarts House Sorting with KNN"
subtitle: "Build your own Sorting Hat using machine learning!"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
Sys.setlocale("LC_CTYPE", "en_US.UTF-8")
```

## ğŸ¯ Learning Goals

In this magical spell, you'll learn to:
- ğŸ° Explore Hogwarts student personality data
- ğŸ© Build a KNN model to sort students into houses
- ğŸ“Š Use cross-validation to find the optimal K value
- ğŸ” Evaluate multi-class classification performance
- âœ¨ Create your own student profile and get sorted!

---

## ğŸ“š Load Our Magical Libraries

```{r load-libraries}
# Load our magical libraries
library(tidymodels)  # For machine learning magic
library(tidyverse)   # For data manipulation and visualization
```

---

## ğŸ° Part 1: Welcome to Hogwarts!

Let's explore the magical world of Hogwarts student data!

```{r load-hogwarts-data}
# Load the Hogwarts student data
# This contains personality traits of students and their houses
hogwarts <- read_csv("../datasets/hogwarts_students.csv")

# Explore the magical data
head(hogwarts)
```

```{r explore-structure}
glimpse(hogwarts)
```

```{r count-houses}
# See how many students are in each house
hogwarts %>% count(house)
```

### ğŸ¨ Visualize Student Personalities

Let's see if we can spot patterns in how different houses cluster!

```{r plot-courage-intelligence}
# Visualize personality traits by house
ggplot(hogwarts, aes(x = courage, y = intelligence, color = house)) +
  geom_point(alpha = 0.7, size = 2) +
  labs(title = "Hogwarts Students: Courage vs Intelligence",
       x = "Courage Score", 
       y = "Intelligence Score",
       color = "House") +
  theme_minimal()
```

### ğŸ¤” Think About It!

ğŸ’¡ **Question:** Can you see any patterns? Which house tends to have high courage? High intelligence?

---

## ğŸ© Part 2: Build Your Sorting Hat Model

Time to create our own magical Sorting Hat using machine learning!

### ğŸ“Š Split the Data for Training and Testing

```{r split-data}
# Split the data into training and testing sets
set.seed(456)  # For reproducible magic!

# We'll train on 75% of students, test on 25%
hogwarts_split <- initial_split(hogwarts, prop = 0.75, strata = house)
train_data <- training(hogwarts_split)
test_data <- testing(hogwarts_split)
```

```{r check-splits}
# Check our splits - each house should be represented
cat("Training data distribution:\n")
train_data %>% count(house)

cat("\nTesting data distribution:\n")
test_data %>% count(house)
```

### ğŸ³ Create the Sorting Hat Recipe

Our recipe will prepare the personality data for the Sorting Hat!

```{r create-recipe}
# Create a recipe for preprocessing
# We'll use all personality traits to predict house
hogwarts_recipe <- recipe(house ~ courage + intelligence + ambition + loyalty, data = train_data) %>%
  step_center(all_predictors()) %>%  # Center around 0
  step_scale(all_predictors())       # Scale to standard deviation 1

print("Sorting Hat recipe created!")
print(hogwarts_recipe)
```

---

## ğŸ“Š Part 3: Find the Best K Value Using Cross-Validation

Instead of just guessing what K value to use, let's use cross-validation to find the best one!

### ğŸ”„ Set Up Cross-Validation

```{r setup-cv}
set.seed(789)

# Create cross-validation folds (like 5 mini train/test splits)
cv_folds <- vfold_cv(train_data, v = 5, strata = house)

print("Created 5-fold cross-validation setup!")
```

### ğŸ¤– Create a Tunable KNN Model

```{r create-tunable-model}
# Create a KNN model that we can tune
knn_model <- nearest_neighbor(neighbors = tune()) %>%
  set_engine("kknn") %>%
  set_mode("classification")

# Create our Sorting Hat workflow
sorting_hat_workflow <- workflow() %>%
  add_recipe(hogwarts_recipe) %>%
  add_model(knn_model)

print("Tunable Sorting Hat workflow created!")
```

### âš¡ Test Different K Values

```{r tune-k-values}
# Test different K values (how many "neighbors" to ask)
k_grid <- tibble(neighbors = c(1, 3, 5, 7, 9, 11, 15, 20, 25))

# Tune the model - this finds the best K!
print("Training the Sorting Hat... please wait!")
tune_results <- sorting_hat_workflow %>%
  tune_grid(resamples = cv_folds, grid = k_grid)

print("Tuning complete! ğŸ‰")
```

### ğŸ“ˆ Analyze the Results

```{r analyze-results}
# Collect the results
tune_metrics <- tune_results %>% collect_metrics()

# Show accuracy results
accuracy_results <- tune_metrics %>%
  filter(.metric == "accuracy") %>%
  arrange(desc(mean))

print("K values ranked by accuracy (best first):")
print(accuracy_results)
```

### ğŸ“Š The Elbow Method Visualization

```{r plot-elbow-method}
# Plot the elbow method - K vs accuracy
accuracy_results %>%
  ggplot(aes(x = neighbors, y = mean)) +
  geom_line(color = "blue", linewidth = 1) +
  geom_point(color = "red", size = 3) +
  geom_errorbar(aes(ymin = mean - std_err, ymax = mean + std_err), 
                width = 0.5, alpha = 0.7) +
  labs(title = "Finding the Best K: The Elbow Method",
       subtitle = "Error bars show uncertainty across CV folds",
       x = "Number of Neighbors (K)",
       y = "Accuracy") +
  theme_minimal()
```

---

## ğŸ©âœ¨ Part 4: Train Your Final Sorting Hat

Now let's build our final Sorting Hat with the best K value!

```{r select-best-k}
# Select the best K value
best_k <- accuracy_results %>%
  slice_head(n = 1) %>%
  pull(neighbors)

print(paste("ğŸ‰ Best K value:", best_k))
print(paste("ğŸ¯ Best accuracy:", round(accuracy_results$mean[1], 3)))
```

```{r train-final-model}
# Create final Sorting Hat with best K
final_sorting_hat <- nearest_neighbor(neighbors = best_k) %>%
  set_engine("kknn") %>%
  set_mode("classification")

final_workflow <- workflow() %>%
  add_recipe(hogwarts_recipe) %>%
  add_model(final_sorting_hat)

# Train on all training data
sorting_hat_fit <- final_workflow %>%
  fit(data = train_data)

print("Final Sorting Hat trained and ready! ğŸ©")
```

### ğŸ§ª Test on New Students

```{r test-final-model}
# Make predictions on test set (students the hat has never seen!)
test_predictions <- predict(sorting_hat_fit, test_data) %>%
  bind_cols(test_data)

# Evaluate how well our Sorting Hat performs
final_metrics <- test_predictions %>%
  metrics(truth = house, estimate = .pred_class)

print("ğŸ† Final Sorting Hat performance on new students:")
print(final_metrics)
```

### ğŸ“Š Confusion Matrix Analysis

```{r confusion-matrix}
# Show confusion matrix - which houses get confused?
print("ğŸ“Š Confusion Matrix (where mistakes happen):")
confusion_matrix <- test_predictions %>%
  conf_mat(truth = house, estimate = .pred_class)
print(confusion_matrix)
```

---

## ğŸ¨ Part 5: Visualize Your Sorting Hat Results

Let's create beautiful visualizations to understand our Sorting Hat's decisions!

### ğŸ¯ Courage vs Intelligence View

```{r plot-courage-intelligence-pred}
# Plot actual vs predicted houses
ggplot(test_predictions, aes(x = courage, y = intelligence)) +
  geom_point(aes(color = house, shape = .pred_class), 
             size = 3, alpha = 0.8) +
  labs(title = "Sorting Hat Predictions vs Reality",
       subtitle = "Color = Actual House, Shape = Predicted House",
       x = "Courage", 
       y = "Intelligence",
       color = "Actual House",
       shape = "Predicted House") +
  theme_minimal()
```

### âš”ï¸ Ambition vs Loyalty View

```{r plot-ambition-loyalty}
# Another view: ambition vs loyalty
ggplot(test_predictions, aes(x = ambition, y = loyalty)) +
  geom_point(aes(color = house, shape = .pred_class), 
             size = 3, alpha = 0.8) +
  labs(title = "Sorting Hat: Ambition vs Loyalty",
       subtitle = "Color = Actual House, Shape = Predicted House",
       x = "Ambition", 
       y = "Loyalty",
       color = "Actual House",
       shape = "Predicted House") +
  theme_minimal()
```

---

## ğŸ†• Part 6: Sort Some New Students!

Let's test our Sorting Hat on some brand new students!

```{r new-students}
# Create some new mystery students to sort
new_students <- tibble(
  name = c("Alex Smith", "Jordan Lee", "Casey Brown", "Riley Johnson"),
  courage = c(8.5, 6.2, 7.8, 9.1),
  intelligence = c(7.1, 9.3, 6.9, 7.5),
  ambition = c(6.8, 7.2, 8.9, 6.3),
  loyalty = c(8.2, 8.8, 6.1, 8.9)
)

print("ğŸ“ New students waiting to be sorted:")
print(new_students)
```

```{r sort-new-students}
# Use our Sorting Hat to predict their houses!
new_predictions <- predict(sorting_hat_fit, new_students) %>%
  bind_cols(new_students)

print("ğŸ  Sorting Hat decisions:")
new_predictions %>%
  select(name, .pred_class, everything()) %>%
  rename(predicted_house = .pred_class) %>%
  print()
```

---

## ğŸ§  Part 7: Understanding Your Sorting Hat

Let's dive deeper into how our Sorting Hat makes decisions!

### ğŸ° House Personality Profiles

```{r house-profiles}
# Let's see which traits matter most for each house
trait_summary <- train_data %>%
  group_by(house) %>%
  summarise(
    avg_courage = round(mean(courage), 2),
    avg_intelligence = round(mean(intelligence), 2),
    avg_ambition = round(mean(ambition), 2),
    avg_loyalty = round(mean(loyalty), 2),
    .groups = 'drop'
  )

print("ğŸ° Average traits by house:")
print(trait_summary)
```

### ğŸ“Š How Much Better Than Random Guessing?

```{r baseline-comparison}
# Calculate baseline accuracy (what if we always guessed the most common house?)
most_common_house <- train_data %>% 
  count(house) %>% 
  arrange(desc(n)) %>% 
  slice_head(n = 1) %>% 
  pull(house)

baseline_accuracy <- test_data %>% 
  count(house) %>% 
  filter(house == most_common_house) %>% 
  pull(n) / nrow(test_data)

final_accuracy <- final_metrics %>% 
  filter(.metric == "accuracy") %>% 
  pull(.estimate)

print(paste("ğŸ“Š Baseline accuracy (always guess", most_common_house, "):", round(baseline_accuracy, 3)))
print(paste("ğŸ¯ Our Sorting Hat accuracy:", round(final_accuracy, 3)))
print(paste("âœ¨ Improvement over baseline:", round(final_accuracy - baseline_accuracy, 3)))
```

---

## ğŸˆ Your Turn: Analysis Questions!

### ğŸ¤” Think About These Questions:

```{r analysis-questions}
print("ğŸ¤” Think about these questions:")
print("1. Which house was hardest for the Sorting Hat to predict correctly?")
print("2. Why do you think some houses get confused with others?")
print("3. If you were a Hogwarts student, what would your traits be?")
```

### âœ¨ Challenge: Create Your Own Student Profile!

```{r create-my-profile}
# Create your own student profile!
my_profile <- tibble(
  name = "Your Name Here",
  courage = 8.0,      # Rate yourself 1-10
  intelligence = 8.0, # Rate yourself 1-10  
  ambition = 8.0,     # Rate yourself 1-10
  loyalty = 8.0       # Rate yourself 1-10
)

print("ğŸ  Where would YOU be sorted?")
my_house <- predict(sorting_hat_fit, my_profile) %>%
  bind_cols(my_profile)
print(my_house)
```

### ğŸ¯ Advanced Challenges

Try these if you want to go further:

1. **âœ¨ Challenge 1:** Can you improve the accuracy by adding new features or transforming existing ones?

2. **âœ¨ Challenge 2:** Create a web app interface where people can input their traits and get sorted!

3. **âœ¨ Challenge 3:** Compare KNN with other classification algorithms like decision trees or logistic regression.

---

## ğŸ’¡ Key Learning Points

ğŸ¯ **What we learned today:**

- **KNN classification can sort students into houses based on personality** - it finds similar students and sees what houses they're in
- **Cross-validation helps us find the best K value** - it's like testing our model multiple times to be sure
- **Some houses are easier to predict than others** - some have more distinct personality patterns
- **Standardizing features is crucial when traits have similar scales** - ensures fair comparison across all traits
- **The elbow method shows us how K affects performance** - helps us choose the sweet spot
- **Even the Sorting Hat makes mistakes sometimes!** - no model is perfect, and that's okay

ğŸ”® **Remember:** Machine learning models learn patterns from data, just like how the real Sorting Hat learned from centuries of students. The more data we have, the better our predictions become!