---
title: "🔮 Day 5 - Spell 2: Hogwarts House Sorting with KNN"
subtitle: "Build your own Sorting Hat using machine learning!"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
Sys.setlocale("LC_CTYPE", "en_US.UTF-8")
```

## 🎯 Learning Goals

In this magical spell, you'll learn to:
- 🏰 Explore Hogwarts student personality data
- 🎩 Build a KNN model to sort students into houses
- 📊 Use cross-validation to find the optimal K value
- 🔍 Evaluate multi-class classification performance
- ✨ Create your own student profile and get sorted!

---

## 📚 Load Our Magical Libraries

```{r load-libraries}
# Load our magical libraries
library(tidymodels)  # For machine learning magic
library(tidyverse)   # For data manipulation and visualization
```

---

## 🏰 Part 1: Welcome to Hogwarts!

Let's explore the magical world of Hogwarts student data!

```{r load-hogwarts-data}
# Load the Hogwarts student data
# This contains personality traits of students and their houses
hogwarts <- read_csv("../datasets/hogwarts_students.csv")

# Explore the magical data
head(hogwarts)
```

```{r explore-structure}
glimpse(hogwarts)
```

```{r count-houses}
# See how many students are in each house
hogwarts %>% count(house)
```

### 🎨 Visualize Student Personalities

Let's see if we can spot patterns in how different houses cluster!

```{r plot-courage-intelligence}
# Visualize personality traits by house
ggplot(hogwarts, aes(x = courage, y = intelligence, color = house)) +
  geom_point(alpha = 0.7, size = 2) +
  labs(title = "Hogwarts Students: Courage vs Intelligence",
       x = "Courage Score", 
       y = "Intelligence Score",
       color = "House") +
  theme_minimal()
```

### 🤔 Think About It!

💡 **Question:** Can you see any patterns? Which house tends to have high courage? High intelligence?

---

## 🎩 Part 2: Build Your Sorting Hat Model

Time to create our own magical Sorting Hat using machine learning!

### 📊 Split the Data for Training and Testing

```{r split-data}
# Split the data into training and testing sets
set.seed(456)  # For reproducible magic!

# We'll train on 75% of students, test on 25%
hogwarts_split <- initial_split(hogwarts, prop = 0.75, strata = house)
train_data <- training(hogwarts_split)
test_data <- testing(hogwarts_split)
```

```{r check-splits}
# Check our splits - each house should be represented
cat("Training data distribution:\n")
train_data %>% count(house)

cat("\nTesting data distribution:\n")
test_data %>% count(house)
```

### 🍳 Create the Sorting Hat Recipe

Our recipe will prepare the personality data for the Sorting Hat!

```{r create-recipe}
# Create a recipe for preprocessing
# We'll use all personality traits to predict house
hogwarts_recipe <- recipe(house ~ courage + intelligence + ambition + loyalty, data = train_data) %>%
  step_center(all_predictors()) %>%  # Center around 0
  step_scale(all_predictors())       # Scale to standard deviation 1

print("Sorting Hat recipe created!")
print(hogwarts_recipe)
```

---

## 📊 Part 3: Find the Best K Value Using Cross-Validation

Instead of just guessing what K value to use, let's use cross-validation to find the best one!

### 🔄 Set Up Cross-Validation

```{r setup-cv}
set.seed(789)

# Create cross-validation folds (like 5 mini train/test splits)
cv_folds <- vfold_cv(train_data, v = 5, strata = house)

print("Created 5-fold cross-validation setup!")
```

### 🤖 Create a Tunable KNN Model

```{r create-tunable-model}
# Create a KNN model that we can tune
knn_model <- nearest_neighbor(neighbors = tune()) %>%
  set_engine("kknn") %>%
  set_mode("classification")

# Create our Sorting Hat workflow
sorting_hat_workflow <- workflow() %>%
  add_recipe(hogwarts_recipe) %>%
  add_model(knn_model)

print("Tunable Sorting Hat workflow created!")
```

### ⚡ Test Different K Values

```{r tune-k-values}
# Test different K values (how many "neighbors" to ask)
k_grid <- tibble(neighbors = c(1, 3, 5, 7, 9, 11, 15, 20, 25))

# Tune the model - this finds the best K!
print("Training the Sorting Hat... please wait!")
tune_results <- sorting_hat_workflow %>%
  tune_grid(resamples = cv_folds, grid = k_grid)

print("Tuning complete! 🎉")
```

### 📈 Analyze the Results

```{r analyze-results}
# Collect the results
tune_metrics <- tune_results %>% collect_metrics()

# Show accuracy results
accuracy_results <- tune_metrics %>%
  filter(.metric == "accuracy") %>%
  arrange(desc(mean))

print("K values ranked by accuracy (best first):")
print(accuracy_results)
```

### 📊 The Elbow Method Visualization

```{r plot-elbow-method}
# Plot the elbow method - K vs accuracy
accuracy_results %>%
  ggplot(aes(x = neighbors, y = mean)) +
  geom_line(color = "blue", linewidth = 1) +
  geom_point(color = "red", size = 3) +
  geom_errorbar(aes(ymin = mean - std_err, ymax = mean + std_err), 
                width = 0.5, alpha = 0.7) +
  labs(title = "Finding the Best K: The Elbow Method",
       subtitle = "Error bars show uncertainty across CV folds",
       x = "Number of Neighbors (K)",
       y = "Accuracy") +
  theme_minimal()
```

---

## 🎩✨ Part 4: Train Your Final Sorting Hat

Now let's build our final Sorting Hat with the best K value!

```{r select-best-k}
# Select the best K value
best_k <- accuracy_results %>%
  slice_head(n = 1) %>%
  pull(neighbors)

print(paste("🎉 Best K value:", best_k))
print(paste("🎯 Best accuracy:", round(accuracy_results$mean[1], 3)))
```

```{r train-final-model}
# Create final Sorting Hat with best K
final_sorting_hat <- nearest_neighbor(neighbors = best_k) %>%
  set_engine("kknn") %>%
  set_mode("classification")

final_workflow <- workflow() %>%
  add_recipe(hogwarts_recipe) %>%
  add_model(final_sorting_hat)

# Train on all training data
sorting_hat_fit <- final_workflow %>%
  fit(data = train_data)

print("Final Sorting Hat trained and ready! 🎩")
```

### 🧪 Test on New Students

```{r test-final-model}
# Make predictions on test set (students the hat has never seen!)
test_predictions <- predict(sorting_hat_fit, test_data) %>%
  bind_cols(test_data)

# Evaluate how well our Sorting Hat performs
final_metrics <- test_predictions %>%
  metrics(truth = house, estimate = .pred_class)

print("🏆 Final Sorting Hat performance on new students:")
print(final_metrics)
```

### 📊 Confusion Matrix Analysis

```{r confusion-matrix}
# Show confusion matrix - which houses get confused?
print("📊 Confusion Matrix (where mistakes happen):")
confusion_matrix <- test_predictions %>%
  conf_mat(truth = house, estimate = .pred_class)
print(confusion_matrix)
```

---

## 🎨 Part 5: Visualize Your Sorting Hat Results

Let's create beautiful visualizations to understand our Sorting Hat's decisions!

### 🎯 Courage vs Intelligence View

```{r plot-courage-intelligence-pred}
# Plot actual vs predicted houses
ggplot(test_predictions, aes(x = courage, y = intelligence)) +
  geom_point(aes(color = house, shape = .pred_class), 
             size = 3, alpha = 0.8) +
  labs(title = "Sorting Hat Predictions vs Reality",
       subtitle = "Color = Actual House, Shape = Predicted House",
       x = "Courage", 
       y = "Intelligence",
       color = "Actual House",
       shape = "Predicted House") +
  theme_minimal()
```

### ⚔️ Ambition vs Loyalty View

```{r plot-ambition-loyalty}
# Another view: ambition vs loyalty
ggplot(test_predictions, aes(x = ambition, y = loyalty)) +
  geom_point(aes(color = house, shape = .pred_class), 
             size = 3, alpha = 0.8) +
  labs(title = "Sorting Hat: Ambition vs Loyalty",
       subtitle = "Color = Actual House, Shape = Predicted House",
       x = "Ambition", 
       y = "Loyalty",
       color = "Actual House",
       shape = "Predicted House") +
  theme_minimal()
```

---

## 🆕 Part 6: Sort Some New Students!

Let's test our Sorting Hat on some brand new students!

```{r new-students}
# Create some new mystery students to sort
new_students <- tibble(
  name = c("Alex Smith", "Jordan Lee", "Casey Brown", "Riley Johnson"),
  courage = c(8.5, 6.2, 7.8, 9.1),
  intelligence = c(7.1, 9.3, 6.9, 7.5),
  ambition = c(6.8, 7.2, 8.9, 6.3),
  loyalty = c(8.2, 8.8, 6.1, 8.9)
)

print("🎓 New students waiting to be sorted:")
print(new_students)
```

```{r sort-new-students}
# Use our Sorting Hat to predict their houses!
new_predictions <- predict(sorting_hat_fit, new_students) %>%
  bind_cols(new_students)

print("🏠 Sorting Hat decisions:")
new_predictions %>%
  select(name, .pred_class, everything()) %>%
  rename(predicted_house = .pred_class) %>%
  print()
```

---

## 🧠 Part 7: Understanding Your Sorting Hat

Let's dive deeper into how our Sorting Hat makes decisions!

### 🏰 House Personality Profiles

```{r house-profiles}
# Let's see which traits matter most for each house
trait_summary <- train_data %>%
  group_by(house) %>%
  summarise(
    avg_courage = round(mean(courage), 2),
    avg_intelligence = round(mean(intelligence), 2),
    avg_ambition = round(mean(ambition), 2),
    avg_loyalty = round(mean(loyalty), 2),
    .groups = 'drop'
  )

print("🏰 Average traits by house:")
print(trait_summary)
```

### 📊 How Much Better Than Random Guessing?

```{r baseline-comparison}
# Calculate baseline accuracy (what if we always guessed the most common house?)
most_common_house <- train_data %>% 
  count(house) %>% 
  arrange(desc(n)) %>% 
  slice_head(n = 1) %>% 
  pull(house)

baseline_accuracy <- test_data %>% 
  count(house) %>% 
  filter(house == most_common_house) %>% 
  pull(n) / nrow(test_data)

final_accuracy <- final_metrics %>% 
  filter(.metric == "accuracy") %>% 
  pull(.estimate)

print(paste("📊 Baseline accuracy (always guess", most_common_house, "):", round(baseline_accuracy, 3)))
print(paste("🎯 Our Sorting Hat accuracy:", round(final_accuracy, 3)))
print(paste("✨ Improvement over baseline:", round(final_accuracy - baseline_accuracy, 3)))
```

---

## 🎈 Your Turn: Analysis Questions!

### 🤔 Think About These Questions:

```{r analysis-questions}
print("🤔 Think about these questions:")
print("1. Which house was hardest for the Sorting Hat to predict correctly?")
print("2. Why do you think some houses get confused with others?")
print("3. If you were a Hogwarts student, what would your traits be?")
```

### ✨ Challenge: Create Your Own Student Profile!

```{r create-my-profile}
# Create your own student profile!
my_profile <- tibble(
  name = "Your Name Here",
  courage = 8.0,      # Rate yourself 1-10
  intelligence = 8.0, # Rate yourself 1-10  
  ambition = 8.0,     # Rate yourself 1-10
  loyalty = 8.0       # Rate yourself 1-10
)

print("🏠 Where would YOU be sorted?")
my_house <- predict(sorting_hat_fit, my_profile) %>%
  bind_cols(my_profile)
print(my_house)
```

### 🎯 Advanced Challenges

Try these if you want to go further:

1. **✨ Challenge 1:** Can you improve the accuracy by adding new features or transforming existing ones?

2. **✨ Challenge 2:** Create a web app interface where people can input their traits and get sorted!

3. **✨ Challenge 3:** Compare KNN with other classification algorithms like decision trees or logistic regression.

---

## 💡 Key Learning Points

🎯 **What we learned today:**

- **KNN classification can sort students into houses based on personality** - it finds similar students and sees what houses they're in
- **Cross-validation helps us find the best K value** - it's like testing our model multiple times to be sure
- **Some houses are easier to predict than others** - some have more distinct personality patterns
- **Standardizing features is crucial when traits have similar scales** - ensures fair comparison across all traits
- **The elbow method shows us how K affects performance** - helps us choose the sweet spot
- **Even the Sorting Hat makes mistakes sometimes!** - no model is perfect, and that's okay

🔮 **Remember:** Machine learning models learn patterns from data, just like how the real Sorting Hat learned from centuries of students. The more data we have, the better our predictions become!